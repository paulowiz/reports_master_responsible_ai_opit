
@online{noauthor_visual_nodate,
	title = {Visual Geometry Group - University of Oxford},
	url = {https://www.robots.ox.ac.uk/~vgg/data/pets/},
	urldate = {2025-03-02},
	file = {Visual Geometry Group - University of Oxford:files/123/pets.html:text/html},
}

@online{noauthor_oxfordiiitpet_nodate,
	title = {{OxfordIIITPet} — Torchvision main documentation},
	url = {https://pytorch.org/vision/main/generated/torchvision.datasets.OxfordIIITPet.html},
	urldate = {2025-03-02},
	file = {OxfordIIITPet — Torchvision main documentation:files/125/torchvision.datasets.OxfordIIITPet.html:text/html},
}

@online{noauthor_discussion_nodate,
	title = {[Discussion] Why normalise according to {ImageNet} mean and std dev for transfer learning? - vision - {PyTorch} Forums},
	url = {https://discuss.pytorch.org/t/discussion-why-normalise-according-to-imagenet-mean-and-std-dev-for-transfer-learning/115670/6},
	urldate = {2025-03-02},
}

@online{zihaozhihao_answer_2019,
	title = {Answer to "Why Pytorch officially use mean=[0.485, 0.456, 0.406] and std=[0.229, 0.224, 0.225] to normalize images?"},
	url = {https://stackoverflow.com/a/58151903},
	shorttitle = {Answer to "Why Pytorch officially use mean=[0.485, 0.456, 0.406] and std=[0.229, 0.224, 0.225] to normalize images?},
	titleaddon = {Stack Overflow},
	author = {zihaozhihao},
	urldate = {2025-03-02},
	date = {2019-09-29},
	file = {Snapshot:files/128/why-pytorch-officially-use-mean-0-485-0-456-0-406-and-std-0-229-0-224-0-2.html:text/html},
}

@online{noauthor_how_2020,
	title = {How {ReLU} and Dropout Layers Work in {CNNs} {\textbar} Baeldung on Computer Science},
	url = {https://www.baeldung.com/cs/ml-relu-dropout-layers},
	abstract = {Study two fundamental components of Convolutional Neural Networks - the Rectified Linear Unit and the Dropout Layer.},
	urldate = {2025-03-02},
	date = {2020-05-30},
	langid = {american},
	file = {Snapshot:files/130/ml-relu-dropout-layers.html:text/html},
}

@online{noauthor_nvidia_nodate,
	title = {{NVIDIA} A100 {GPUs} Power the Modern Data Center},
	url = {https://www.nvidia.com/en-us/data-center/a100/},
	abstract = {The fastest data center platform for {AI} and {HPC}.},
	titleaddon = {{NVIDIA}},
	urldate = {2025-03-02},
	langid = {english},
	file = {Snapshot:files/132/a100.html:text/html},
}

@online{noauthor_google_nodate,
	title = {Google Colab},
	url = {https://research.google.com/colaboratory/faq.html},
	urldate = {2025-03-02},
	file = {Google Colab:files/134/faq.html:text/html},
}

@online{noauthor_pytorch_nodate,
	title = {Pytorch Optimizer Steplr Overview {\textbar} Restackio},
	url = {https://www.restack.io/p/pytorch-answer-steplr-optimizer},
	abstract = {Explore the Steplr optimizer in Pytorch for effective learning rate scheduling and improved model training. {\textbar} Restackio},
	urldate = {2025-03-02},
	langid = {english},
	file = {Snapshot:files/136/pytorch-answer-steplr-optimizer.html:text/html},
}

@online{noauthor_what_nodate,
	title = {What is Adam Optimizer?},
	url = {https://www.geeksforgeeks.org/adam-optimizer/},
	abstract = {A Computer Science portal for geeks. It contains well written, well thought and well explained computer science and programming articles, quizzes and practice/competitive programming/company interview Questions.},
	titleaddon = {{GeeksforGeeks}},
	urldate = {2025-03-02},
	langid = {american},
	note = {Section: Deep Learning},
	file = {Snapshot:files/138/adam-optimizer.html:text/html},
}

@online{noauthor_vgg_nodate,
	title = {{VGG} — Torchvision main documentation},
	url = {https://pytorch.org/vision/main/models/vgg.html},
	urldate = {2025-03-16},
}

@misc{simonyan_very_2015,
	title = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
	url = {http://arxiv.org/abs/1409.1556},
	doi = {10.48550/arXiv.1409.1556},
	abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our {ImageNet} Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing {ConvNet} models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
	number = {{arXiv}:1409.1556},
	publisher = {{arXiv}},
	author = {Simonyan, Karen and Zisserman, Andrew},
	urldate = {2025-03-16},
	date = {2015-04-10},
	eprinttype = {arxiv},
	eprint = {1409.1556 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:files/151/Simonyan and Zisserman - 2015 - Very Deep Convolutional Networks for Large-Scale I.pdf:application/pdf;Snapshot:files/152/1409.html:text/html},
}
